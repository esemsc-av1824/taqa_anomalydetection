{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70397dd7",
   "metadata": {},
   "source": [
    "# Data Profiling and Selection-Guiding Summaries\n",
    "\n",
    "This notebook:\n",
    "- Reads all CSVs in `raw/` (ignoring `Zone.Identifier`), extracts `tool_id` and `signal` from filename.\n",
    "- Autodetects timestamp and value columns, standardizes to long format `{timestamp, tool_id, signal, value}`.\n",
    "- Profiles streams, infers sampling intervals, resamples per tool to dominant frequency, builds wide and long tables.\n",
    "- Saves artifacts under `eval/`:\n",
    "  - `schema_inventory.csv`\n",
    "  - `wide_by_tool.parquet`\n",
    "  - `long_all.parquet`\n",
    "  - `predictability.csv`, `ts_characteristics.csv`, `corr_matrix.csv`, `vif.csv`, `outlier_rate.csv`\n",
    "- Prints compact selection tables at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ef3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pyarrow) (1.24.4)\n",
      "Downloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m590.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m590.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Successfully installed pyarrow-17.0.0\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (1.24.4)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (681 bytes)\n",
      "Requirement already satisfied: fsspec in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (681 bytes)\n",
      "Requirement already satisfied: fsspec in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2024.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mRequirement already satisfied: six>=1.5 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2024.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m664.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m664.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cramjam-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading cramjam-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m594.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:03\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m594.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.2.0\n",
      "Successfully installed cramjam-2.11.0 fastparquet-2024.2.0\n",
      "Requirement already satisfied: scikit-learn in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/av1824/miniconda3/envs/dsml/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Root: /home/av1824/TAQA\n",
      "RAW dir: /home/av1824/TAQA/raw  exists=True\n",
      "Eval dir: /home/av1824/TAQA/eval\n",
      "Root: /home/av1824/TAQA\n",
      "RAW dir: /home/av1824/TAQA/raw  exists=True\n",
      "Eval dir: /home/av1824/TAQA/eval\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import sys, os, re, math, warnings, json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional installs for parquet and stats\n",
    "def ensure_packages(pkgs):\n",
    "    import importlib, subprocess\n",
    "    for name in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(name)\n",
    "        except Exception:\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", name])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to install {name}: {e}\")\n",
    "\n",
    "ensure_packages([\"pyarrow\", \"fastparquet\", \"scikit-learn\", \"statsmodels\", \"tqdm\"])\n",
    "\n",
    "# Re-import after potential installation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as sm_vif\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "RAW_DIR = ROOT / \"raw\"\n",
    "EVAL_DIR = ROOT / \"eval\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"RAW dir: {RAW_DIR}  exists={RAW_DIR.exists()}\")\n",
    "print(f\"Eval dir: {EVAL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7319b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "import pycaret\n",
    "print(pycaret.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b94fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: filename parsing, timestamp/value detection, parquet writer\n",
    "FILENAME_RE = re.compile(r\"Tool\\.([^.]+)\\.[^.]+\\.([^.]+)\\.csv$\", re.IGNORECASE)\n",
    "TS_CANDIDATES = [\n",
    "    \"timestamp\",\"time\",\"datetime\",\"date\",\"Date\",\"Time\",\"Timestamp\"\n",
    "]\n",
    "\n",
    "def extract_meta_from_path(path: Path):\n",
    "    name = path.name\n",
    "    m = FILENAME_RE.search(name)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    tool_id, signal = m.group(1), m.group(2)\n",
    "    return tool_id, signal\n",
    "\n",
    "# Try to detect timestamp column by name or by parsability (>90%)\n",
    "def detect_timestamp_column(df: pd.DataFrame):\n",
    "    # by candidate names (case-insensitive)\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for key in TS_CANDIDATES:\n",
    "        if key.lower() in lower_map:\n",
    "            col = lower_map[key.lower()]\n",
    "            ts = pd.to_datetime(df[col], errors='coerce', utc=False)\n",
    "            valid = ts.notna().mean()\n",
    "            if valid >= 0.90:\n",
    "                return col, ts\n",
    "    # heuristic: first column with >90% parse rate\n",
    "    for col in df.columns:\n",
    "        ts = pd.to_datetime(df[col], errors='coerce', utc=False)\n",
    "        if ts.notna().mean() >= 0.90:\n",
    "            return col, ts\n",
    "    return None, None\n",
    "\n",
    "# Find first numeric column after the timestamp column\n",
    "def detect_value_column(df: pd.DataFrame, ts_col: str):\n",
    "    cols = list(df.columns)\n",
    "    start_idx = 0 if ts_col not in cols else cols.index(ts_col) + 1\n",
    "    for col in cols[start_idx:]:\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        if s.notna().mean() >= 0.90:\n",
    "            return col, s\n",
    "    # fallback: any prior column\n",
    "    for col in cols:\n",
    "        if col == ts_col: continue\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        if s.notna().mean() >= 0.90:\n",
    "            return col, s\n",
    "    return None, None\n",
    "\n",
    "# Robust parquet writer\n",
    "def to_parquet_safe(df: pd.DataFrame, path: Path, index=False):\n",
    "    try:\n",
    "        df.to_parquet(path, index=index, engine=\"pyarrow\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            df.to_parquet(path, index=index, engine=\"fastparquet\")\n",
    "        except Exception as e:\n",
    "            # final fallback to CSV next to it\n",
    "            csv_path = path.with_suffix(\".csv\")\n",
    "            df.to_csv(csv_path, index=index)\n",
    "            print(f\"Parquet write failed, wrote CSV instead: {csv_path} ({e})\")\n",
    "\n",
    "# Sampling interval (median) in seconds\n",
    "def median_sampling_seconds(ts: pd.Series):\n",
    "    ts = pd.to_datetime(ts)\n",
    "    ts = ts.sort_values().drop_duplicates()\n",
    "    if ts.size < 2:\n",
    "        return np.nan\n",
    "    dt = ts.diff().dropna().dt.total_seconds()\n",
    "    if dt.empty:\n",
    "        return np.nan\n",
    "    return float(np.median(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767d90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: 54\n"
     ]
    }
   ],
   "source": [
    "# Reader producing standardized long records and schema rows\n",
    "\n",
    "def read_csv_standardize(path: Path):\n",
    "    tool_id, signal = extract_meta_from_path(path)\n",
    "    if not tool_id or not signal:\n",
    "        return None, None\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, low_memory=False, encoding='latin-1')\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "\n",
    "    ts_col, ts_parsed = detect_timestamp_column(df)\n",
    "    if not ts_col:\n",
    "        return None, None\n",
    "\n",
    "    val_col, val_parsed = detect_value_column(df, ts_col)\n",
    "    if not val_col:\n",
    "        return None, None\n",
    "\n",
    "    # Build standardized frame\n",
    "    ts = pd.to_datetime(ts_parsed, errors='coerce')\n",
    "    val = pd.to_numeric(df[val_col], errors='coerce')\n",
    "\n",
    "    # Schema metrics prior to dropping\n",
    "    rows_total = len(df)\n",
    "    missing_pct = float(val.isna().mean() * 100.0)\n",
    "\n",
    "    frame = pd.DataFrame({\n",
    "        'timestamp': ts,\n",
    "        'tool_id': tool_id,\n",
    "        'signal': signal,\n",
    "        'value': val,\n",
    "    }).dropna(subset=['timestamp'])\n",
    "\n",
    "    # Basic cleanup & order\n",
    "    frame = frame.sort_values('timestamp')\n",
    "    # remove exact duplicate timestamp rows for the same tool/signal\n",
    "    frame = frame.drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "    # Sampling inference\n",
    "    freq_s = median_sampling_seconds(frame['timestamp'])\n",
    "\n",
    "    start = frame['timestamp'].min()\n",
    "    end   = frame['timestamp'].max()\n",
    "\n",
    "    schema = {\n",
    "        'file': str(path.relative_to(ROOT)),\n",
    "        'tool_id': tool_id,\n",
    "        'signal': signal,\n",
    "        'rows': int(rows_total),\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'inferred_freq_s': freq_s,\n",
    "        'value_mean': float(frame['value'].mean(skipna=True)) if frame['value'].notna().any() else np.nan,\n",
    "        'value_std': float(frame['value'].std(skipna=True)) if frame['value'].notna().any() else np.nan,\n",
    "        'missing_pct': missing_pct,\n",
    "    }\n",
    "    return frame, schema\n",
    "\n",
    "# Collect files\n",
    "all_csvs = sorted([p for p in RAW_DIR.rglob('*.csv') if 'Zone.Identifier' not in p.name])\n",
    "print(f\"Found CSV files: {len(all_csvs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6d5609",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m records \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m schema_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(all_csvs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading & standardizing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     frame, schema \u001b[38;5;241m=\u001b[39m read_csv_standardize(p)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# Build long table and schema inventory\n",
    "records = []\n",
    "schema_rows = []\n",
    "\n",
    "for p in tqdm(all_csvs, desc=\"Reading & standardizing\"):\n",
    "    frame, schema = read_csv_standardize(p)\n",
    "    if frame is None:\n",
    "        continue\n",
    "    # keep only rows where value is present\n",
    "    records.append(frame)\n",
    "    schema_rows.append(schema)\n",
    "\n",
    "if not records:\n",
    "    print(\"No valid CSVs found.\")\n",
    "    long_df = pd.DataFrame(columns=['timestamp','tool_id','signal','value'])\n",
    "else:\n",
    "    long_df = pd.concat(records, ignore_index=True)\n",
    "\n",
    "schema_df = pd.DataFrame(schema_rows)\n",
    "\n",
    "# Save schema inventory\n",
    "schema_path = EVAL_DIR / 'schema_inventory.csv'\n",
    "schema_df.to_csv(schema_path, index=False)\n",
    "print(f\"Saved: {schema_path}  rows={len(schema_df)}\")\n",
    "\n",
    "# Save long table (raw long, not resampled)\n",
    "long_path = EVAL_DIR / 'long_all.parquet'\n",
    "to_parquet_safe(long_df, long_path, index=False)\n",
    "print(f\"Saved: {long_path}  rows={len(long_df)}\")\n",
    "\n",
    "long_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff583b32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m wide_tables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m resampled_long \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_id, g \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlong_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# compute per-signal median intervals\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     sig_freqs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sig, gg \u001b[38;5;129;01min\u001b[39;00m g\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Resample per tool to dominant frequency and build wide tables\n",
    "\n",
    "def dominant_frequency_seconds(freq_list):\n",
    "    # round to nearest second and take mode\n",
    "    vals = [int(round(f)) for f in freq_list if pd.notna(f) and np.isfinite(f) and f > 0]\n",
    "    if not vals:\n",
    "        return 1\n",
    "    cnt = Counter(vals)\n",
    "    # Find the (count, value) pair with highest count (lowest negative count), tie-breaker: smallest value\n",
    "    best = min(((-c, v), v) for v, c in cnt.items())\n",
    "    return best[1]\n",
    "\n",
    "wide_tables = []\n",
    "resampled_long = []\n",
    "\n",
    "for tool_id, g in long_df.groupby('tool_id'):\n",
    "    # compute per-signal median intervals\n",
    "    sig_freqs = {}\n",
    "    for sig, gg in g.groupby('signal'):\n",
    "        sig_freqs[sig] = median_sampling_seconds(gg['timestamp'])\n",
    "    dom_s = dominant_frequency_seconds(list(sig_freqs.values()))\n",
    "    freq_str = f\"{int(max(1, dom_s))}s\"\n",
    "\n",
    "    # common index across this tool\n",
    "    start = g['timestamp'].min()\n",
    "    end   = g['timestamp'].max()\n",
    "    if pd.isna(start) or pd.isna(end) or start >= end:\n",
    "        continue\n",
    "    idx = pd.date_range(start=start, end=end, freq=freq_str)\n",
    "\n",
    "    # build wide matrix\n",
    "    cols = {}\n",
    "    for sig, gg in g.groupby('signal'):\n",
    "        s = gg.set_index('timestamp')['value'].sort_index()\n",
    "        s = s.resample(freq_str).ffill()\n",
    "        # ensure full coverage on common index\n",
    "        s = s.reindex(idx).ffill()\n",
    "        cols[sig] = s\n",
    "        # also keep resampled long\n",
    "        res = pd.DataFrame({'timestamp': s.index, 'tool_id': tool_id, 'signal': sig, 'value': s.values})\n",
    "        resampled_long.append(res)\n",
    "\n",
    "    wide = pd.DataFrame(cols, index=idx).reset_index().rename(columns={'index': 'timestamp'})\n",
    "    wide.insert(0, 'tool_id', tool_id)\n",
    "    wide_tables.append(wide)\n",
    "\n",
    "# Concatenate\n",
    "if wide_tables:\n",
    "    wide_by_tool = pd.concat(wide_tables, ignore_index=True, sort=False)\n",
    "else:\n",
    "    wide_by_tool = pd.DataFrame()\n",
    "\n",
    "res_long_df = pd.concat(resampled_long, ignore_index=True) if resampled_long else pd.DataFrame(columns=['timestamp','tool_id','signal','value'])\n",
    "\n",
    "# Save artifacts\n",
    "wide_path = EVAL_DIR / 'wide_by_tool.parquet'\n",
    "to_parquet_safe(wide_by_tool, wide_path, index=False)\n",
    "print(f\"Saved: {wide_path}  rows={len(wide_by_tool)}\")\n",
    "\n",
    "res_long_path = EVAL_DIR / 'long_all_resampled.parquet'\n",
    "to_parquet_safe(res_long_df, res_long_path, index=False)\n",
    "print(f\"Saved: {res_long_path}  rows={len(res_long_df)}\")\n",
    "\n",
    "wide_by_tool.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1638a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Predictability score per signal (5-fold CV RFR predicting each signal from others)\n",
    "\n",
    "pred_rows = []\n",
    "MIN_SAMPLES = 200  # skip very small datasets\n",
    "\n",
    "if not wide_by_tool.empty:\n",
    "    for tool_id, w in wide_by_tool.groupby('tool_id'):\n",
    "        # identify signal columns (exclude timestamp/tool_id)\n",
    "        sig_cols = [c for c in w.columns if c not in ('tool_id','timestamp')]\n",
    "        # drop rows where all signals are NaN\n",
    "        W = w.dropna(subset=sig_cols, how='all').copy()\n",
    "        if W.empty:\n",
    "            continue\n",
    "        for target in sig_cols:\n",
    "            X_cols = [c for c in sig_cols if c != target]\n",
    "            sub = W.dropna(subset=[target] + X_cols, how='any')\n",
    "            if len(sub) < max(MIN_SAMPLES, 5):\n",
    "                continue\n",
    "            X = sub[X_cols].values\n",
    "            y = sub[target].values\n",
    "            try:\n",
    "                model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "                cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                scores = cross_val_score(model, X, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "                pred_rows.append({\n",
    "                    'tool_id': tool_id,\n",
    "                    'signal': target,\n",
    "                    'r2_mean': float(np.mean(scores)),\n",
    "                    'r2_std': float(np.std(scores)),\n",
    "                    'n_samples': int(len(sub)),\n",
    "                    'n_features': int(len(X_cols)),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pred_rows.append({\n",
    "                    'tool_id': tool_id,\n",
    "                    'signal': target,\n",
    "                    'r2_mean': np.nan,\n",
    "                    'r2_std': np.nan,\n",
    "                    'n_samples': int(len(sub)),\n",
    "                    'n_features': int(len(X_cols)),\n",
    "                })\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows)\n",
    "\n",
    "# Aggregate overall by signal across tools\n",
    "if not pred_df.empty:\n",
    "    overall = (pred_df.groupby('signal')['r2_mean']\n",
    "                     .mean()\n",
    "                     .reset_index()\n",
    "                     .assign(tool_id='ALL')\n",
    "                     .rename(columns={'r2_mean':'r2_mean'}))\n",
    "    pred_all = pd.concat([pred_df, overall], ignore_index=True, sort=False)\n",
    "else:\n",
    "    pred_all = pred_df.copy()\n",
    "\n",
    "pred_path = EVAL_DIR / 'predictability.csv'\n",
    "pred_all.to_csv(pred_path, index=False)\n",
    "print(f\"Saved: {pred_path}  rows={len(pred_all)}\")\n",
    "\n",
    "pred_all.sort_values(['tool_id','r2_mean'], ascending=[True, False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4aca2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Time-series characteristics per signal: lag1 autocorr, ADF p, top FFT peak period (s), % zeros, % const-run\n",
    "\n",
    "def const_run_fraction(x: np.ndarray, min_len: int = 3):\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    # Identify run lengths of equal consecutive values\n",
    "    diffs = np.diff(x)\n",
    "    same = np.concatenate([[False], diffs == 0])\n",
    "    # compute run lengths\n",
    "    runs = []\n",
    "    run_len = 1\n",
    "    for i in range(1, len(x)):\n",
    "        if x[i] == x[i-1]:\n",
    "            run_len += 1\n",
    "        else:\n",
    "            runs.append(run_len)\n",
    "            run_len = 1\n",
    "    runs.append(run_len)\n",
    "    # fraction of points that belong to runs length >= min_len\n",
    "    eligible = sum([rl for rl in runs if rl >= min_len])\n",
    "    return eligible / float(len(x))\n",
    "\n",
    "char_rows = []\n",
    "\n",
    "if not wide_by_tool.empty:\n",
    "    for tool_id, w in wide_by_tool.groupby('tool_id'):\n",
    "        sig_cols = [c for c in w.columns if c not in ('tool_id','timestamp')]\n",
    "        # Determine dt in seconds from index if evenly spaced; else infer by median\n",
    "        # Use timestamp diffs median\n",
    "        tdiff = pd.to_datetime(w['timestamp']).sort_values().diff().dt.total_seconds().median()\n",
    "        dt = float(tdiff) if pd.notna(tdiff) and tdiff > 0 else 1.0\n",
    "        for sig in sig_cols:\n",
    "            s = pd.to_numeric(w[sig], errors='coerce').dropna()\n",
    "            if len(s) < 50:\n",
    "                continue\n",
    "            try:\n",
    "                lag1 = pd.Series(s.values).autocorr(lag=1)\n",
    "            except Exception:\n",
    "                lag1 = np.nan\n",
    "            try:\n",
    "                adf_p = float(adfuller(s.values, autolag='AIC')[1])\n",
    "            except Exception:\n",
    "                adf_p = np.nan\n",
    "            # FFT peak period\n",
    "            try:\n",
    "                y = s.values.astype(float)\n",
    "                y = y - np.nanmean(y)\n",
    "                n = len(y)\n",
    "                if n > 1:\n",
    "                    freqs = np.fft.rfftfreq(n, d=dt)\n",
    "                    fft = np.abs(np.fft.rfft(y))\n",
    "                    # ignore zero freq\n",
    "                    if freqs.size > 1:\n",
    "                        fft[0] = 0.0\n",
    "                        k = int(np.nanargmax(fft))\n",
    "                        peak_freq = freqs[k]\n",
    "                        peak_period = float(1.0/peak_freq) if peak_freq > 0 else np.nan\n",
    "                    else:\n",
    "                        peak_period = np.nan\n",
    "                else:\n",
    "                    peak_period = np.nan\n",
    "            except Exception:\n",
    "                peak_period = np.nan\n",
    "            # zeros and const runs\n",
    "            arr = s.values\n",
    "            pct_zero = float((arr == 0).mean() * 100.0)\n",
    "            pct_const = float(const_run_fraction(arr, min_len=3) * 100.0)\n",
    "            char_rows.append({\n",
    "                'tool_id': tool_id,\n",
    "                'signal': sig,\n",
    "                'lag1_autocorr': lag1,\n",
    "                'adf_pvalue': adf_p,\n",
    "                'top_fft_peak_period_s': peak_period,\n",
    "                'pct_zeros': pct_zero,\n",
    "                'pct_const_runs': pct_const,\n",
    "            })\n",
    "\n",
    "char_df = pd.DataFrame(char_rows)\n",
    "if not char_df.empty:\n",
    "    overall_char = (char_df.groupby('signal')\n",
    "                           .agg({\n",
    "                               'lag1_autocorr':'mean',\n",
    "                               'adf_pvalue':'mean',\n",
    "                               'top_fft_peak_period_s':'median',\n",
    "                               'pct_zeros':'mean',\n",
    "                               'pct_const_runs':'mean',\n",
    "                            })\n",
    "                           .reset_index()\n",
    "                           .assign(tool_id='ALL'))\n",
    "    char_all = pd.concat([char_df, overall_char], ignore_index=True, sort=False)\n",
    "else:\n",
    "    char_all = char_df.copy()\n",
    "\n",
    "char_path = EVAL_DIR / 'ts_characteristics.csv'\n",
    "char_all.to_csv(char_path, index=False)\n",
    "print(f\"Saved: {char_path}  rows={len(char_all)}\")\n",
    "\n",
    "char_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533312c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cross-signal correlation (Pearson) and VIF\n",
    "\n",
    "corr_rows = []\n",
    "vif_rows = []\n",
    "\n",
    "if not wide_by_tool.empty:\n",
    "    for tool_id, w in wide_by_tool.groupby('tool_id'):\n",
    "        sig_cols = [c for c in w.columns if c not in ('tool_id','timestamp')]\n",
    "        if len(sig_cols) < 2:\n",
    "            continue\n",
    "        dfnum = w[sig_cols].copy()\n",
    "        cmat = dfnum.corr(method='pearson', min_periods=50)\n",
    "        # long format\n",
    "        for i in sig_cols:\n",
    "            for j in sig_cols:\n",
    "                corr_rows.append({'tool_id': tool_id, 'signal_i': i, 'signal_j': j, 'corr': cmat.loc[i, j] if i in cmat.index and j in cmat.columns else np.nan})\n",
    "        # VIF (drop rows with NaN)\n",
    "        X = dfnum.dropna()\n",
    "        if len(X) >= 200 and X.shape[1] >= 2:\n",
    "            # standardize (optional but helps numerical stability)\n",
    "            Xz = (X - X.mean())/X.std(ddof=0)\n",
    "            Xz = Xz.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(Xz) >= 200:\n",
    "                try:\n",
    "                    for k, col in enumerate(Xz.columns):\n",
    "                        vif = sm_vif(Xz.values, k)\n",
    "                        vif_rows.append({'tool_id': tool_id, 'feature': col, 'vif': float(vif)})\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "# Overall correlation across tools (concatenate and compute)\n",
    "if not wide_by_tool.empty:\n",
    "    sig_cols_all = [c for c in wide_by_tool.columns if c not in ('tool_id','timestamp')]\n",
    "    df_all = wide_by_tool[sig_cols_all].copy()\n",
    "    cmat_all = df_all.corr(method='pearson', min_periods=200)\n",
    "    for i in sig_cols_all:\n",
    "        for j in sig_cols_all:\n",
    "            corr_rows.append({'tool_id': 'ALL', 'signal_i': i, 'signal_j': j, 'corr': cmat_all.loc[i, j] if i in cmat_all.index and j in cmat_all.columns else np.nan})\n",
    "    # Overall VIF\n",
    "    Xall = df_all.dropna()\n",
    "    if len(Xall) >= 500 and Xall.shape[1] >= 2:\n",
    "        Xallz = (Xall - Xall.mean())/Xall.std(ddof=0)\n",
    "        Xallz = Xallz.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(Xallz) >= 500:\n",
    "            try:\n",
    "                for k, col in enumerate(Xallz.columns):\n",
    "                    vif = sm_vif(Xallz.values, k)\n",
    "                    vif_rows.append({'tool_id': 'ALL', 'feature': col, 'vif': float(vif)})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "corr_df = pd.DataFrame(corr_rows)\n",
    "vif_df = pd.DataFrame(vif_rows)\n",
    "\n",
    "corr_path = EVAL_DIR / 'corr_matrix.csv'\n",
    "vif_path  = EVAL_DIR / 'vif.csv'\n",
    "\n",
    "corr_df.to_csv(corr_path, index=False)\n",
    "vif_df.to_csv(vif_path, index=False)\n",
    "print(f\"Saved: {corr_path} rows={len(corr_df)}\")\n",
    "print(f\"Saved: {vif_path} rows={len(vif_df)}\")\n",
    "\n",
    "corr_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66de8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Quick outlier rate: z-score > 4 per signal (by tool and overall)\n",
    "\n",
    "out_rows = []\n",
    "if not wide_by_tool.empty:\n",
    "    for tool_id, w in wide_by_tool.groupby('tool_id'):\n",
    "        sig_cols = [c for c in w.columns if c not in ('tool_id','timestamp')]\n",
    "        Z = (w[sig_cols] - w[sig_cols].mean())/w[sig_cols].std(ddof=0)\n",
    "        for c in sig_cols:\n",
    "            s = Z[c].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if s.empty:\n",
    "                continue\n",
    "            rate = float((s.abs() > 4).mean() * 100.0)\n",
    "            out_rows.append({'tool_id': tool_id, 'signal': c, 'outlier_rate_pct': rate, 'n': int(s.size)})\n",
    "\n",
    "# Overall\n",
    "if not wide_by_tool.empty:\n",
    "    sig_cols = [c for c in wide_by_tool.columns if c not in ('tool_id','timestamp')]\n",
    "    Zall = (wide_by_tool[sig_cols] - wide_by_tool[sig_cols].mean())/wide_by_tool[sig_cols].std(ddof=0)\n",
    "    for c in sig_cols:\n",
    "        s = Zall[c].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "        rate = float((s.abs() > 4).mean() * 100.0)\n",
    "        out_rows.append({'tool_id': 'ALL', 'signal': c, 'outlier_rate_pct': rate, 'n': int(s.size)})\n",
    "\n",
    "out_df = pd.DataFrame(out_rows)\n",
    "out_path = EVAL_DIR / 'outlier_rate.csv'\n",
    "out_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path} rows={len(out_df)}\")\n",
    "\n",
    "out_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d098a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Compact selection tables to print\n",
    "\n",
    "# 1) Top 10 most predictable signals (highest R^2)\n",
    "if (EVAL_DIR / 'predictability.csv').exists():\n",
    "    pred = pd.read_csv(EVAL_DIR / 'predictability.csv')\n",
    "    top_pred = (pred[pred['tool_id'] == 'ALL']\n",
    "                .sort_values('r2_mean', ascending=False)\n",
    "                .head(10))\n",
    "    print(\"Top 10 most predictable signals (overall R^2):\")\n",
    "    display(top_pred[['signal','r2_mean']])\n",
    "\n",
    "# 2) Signals with strong temporal structure (high lag-1, low ADF p)\n",
    "if (EVAL_DIR / 'ts_characteristics.csv').exists():\n",
    "    tschar = pd.read_csv(EVAL_DIR / 'ts_characteristics.csv')\n",
    "    overall_ts = tschar[tschar['tool_id'] == 'ALL'].copy()\n",
    "    if not overall_ts.empty:\n",
    "        # rank by lag1 desc and adf_p asc\n",
    "        overall_ts['rank'] = overall_ts['lag1_autocorr'].rank(ascending=False, method='average') + \\\n",
    "                              overall_ts['adf_pvalue'].rank(ascending=True, method='average')\n",
    "        strong_temporal = overall_ts.sort_values('rank').head(10)\n",
    "        print(\"Signals with strongest temporal structure:\")\n",
    "        display(strong_temporal[['signal','lag1_autocorr','adf_pvalue','top_fft_peak_period_s']])\n",
    "\n",
    "# 3) Signals with lowest pairwise correlations to others\n",
    "if (EVAL_DIR / 'corr_matrix.csv').exists():\n",
    "    corr_long = pd.read_csv(EVAL_DIR / 'corr_matrix.csv')\n",
    "    all_corr = corr_long[corr_long['tool_id'] == 'ALL']\n",
    "    # mean absolute corr to others per signal\n",
    "    m = all_corr[all_corr['signal_i'] != all_corr['signal_j']]\n",
    "    mean_abs = (m.assign(abs_corr=m['corr'].abs())\n",
    "                 .groupby('signal_i')['abs_corr']\n",
    "                 .mean()\n",
    "                 .sort_values()\n",
    "                 .reset_index()\n",
    "                 .rename(columns={'signal_i':'signal','abs_corr':'mean_abs_corr'}))\n",
    "    low_corr = mean_abs.head(10)\n",
    "    print(\"Signals with lowest mean absolute correlation to others:\")\n",
    "    display(low_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31216d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import pycaret\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933e88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taqa_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
